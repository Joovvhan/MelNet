{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DelayedRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_hidden):\n",
    "        super(DelayedRNN, self).__init__()\n",
    "\n",
    "        self.t_delay_RNN_x = nn.GRU(input_size=num_hidden, hidden_size=num_hidden, batch_first=True)\n",
    "        self.t_delay_RNN_y = nn.GRU(input_size=num_hidden, hidden_size=num_hidden, batch_first=True)\n",
    "        self.t_delay_RNN_z = nn.GRU(input_size=num_hidden, hidden_size=num_hidden, batch_first=True)\n",
    "\n",
    "        self.W_t = nn.Linear(3 * num_hidden, num_hidden)\n",
    "\n",
    "        self.c_RNN = nn.GRU(input_size=num_hidden, hidden_size=num_hidden, batch_first=True)\n",
    "\n",
    "        self.W_c = nn.Linear(num_hidden, num_hidden)\n",
    "\n",
    "        self.f_delay_RNN = nn.GRU(input_size=num_hidden, hidden_size=num_hidden, batch_first=True)\n",
    "\n",
    "        self.W_f = nn.Linear(num_hidden, num_hidden)\n",
    "\n",
    "    def forward(self, input_h_t, input_h_f, input_h_c):\n",
    "\n",
    "        h_t_x = Variable(torch.zeros(input_h_t.shape))\n",
    "        h_t_y = Variable(torch.zeros(input_h_t.shape))\n",
    "        h_t_z = Variable(torch.zeros(input_h_t.shape))\n",
    "\n",
    "        for i in range(input_h_t.shape[2]):\n",
    "            h_t_x_slice, _ = self.t_delay_RNN_x(input_h_t[:, :, i, :])\n",
    "            h_t_x[:, :, i, :] = h_t_x_slice\n",
    "\n",
    "        reverse_index = np.arange(input_h_t.shape[2] - 1, -1, -1)\n",
    "        for i in range(input_h_t.shape[1]):\n",
    "            h_t_y_slice, _ = self.t_delay_RNN_y(input_h_t[:, i, :, :])\n",
    "            h_t_z_slice, _ = self.t_delay_RNN_z(input_h_t[:, i, reverse_index, :])\n",
    "            h_t_y[:, i, :, :] = h_t_y_slice\n",
    "            h_t_z[:, i, :, :] = h_t_z_slice[:, reverse_index, :]\n",
    "\n",
    "        h_t_concat = torch.cat([h_t_x, h_t_y, h_t_z], 3)\n",
    "\n",
    "        h_t_w = self.W_t(h_t_concat)\n",
    "\n",
    "        output_h_t = torch.add(input_h_t, h_t_w)\n",
    "\n",
    "        h_c_rnn, _ = self.c_RNN(input_h_c)\n",
    "        h_c_w = self.W_c(h_c_rnn)\n",
    "        output_h_c = torch.add(input_h_c, h_c_w)\n",
    "\n",
    "        h_c_expand = output_h_c.view(output_h_c.shape[0], output_h_c.shape[1], 1, output_h_c.shape[2]).repeat(1, 1, 32, 1)\n",
    "        h_f_sum = torch.add(torch.add(input_h_f, output_h_t), h_c_expand)\n",
    "\n",
    "        h_f_ = Variable(torch.zeros(input_h_f.shape))\n",
    "\n",
    "        for i in range(h_f_sum.shape[1]):\n",
    "            h_f_slice, _ = self.f_delay_RNN(h_f_sum[:, i, :, :])\n",
    "            h_f_[:, i, :, :] = h_f_slice\n",
    "\n",
    "        h_f_w = self.W_f(h_f_)\n",
    "\n",
    "        output_h_f = torch.add(input_h_f, h_f_w)\n",
    "\n",
    "        return output_h_t, output_h_f, output_h_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_hidden, num_layer, K):\n",
    "        super(MelNet, self).__init__()\n",
    "        \n",
    "        self.W_t_0 = nn.Linear(1, num_hidden)\n",
    "        self.W_f_0 = nn.Linear(1, num_hidden)\n",
    "        self.W_c_0 = nn.Linear(32, num_hidden)\n",
    "        \n",
    "        self.module_list = nn.ModuleList([DelayedRNN(512) for i in range(num_layer)])\n",
    "\n",
    "        self.W_theta = nn.Linear(num_hidden, 3 * K)\n",
    "        self.pi_softmax = nn.Softmax(dim=3)\n",
    "        self.K = K\n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "        \n",
    "        h_t = self.W_t_0(input_tensor)\n",
    "        h_f = self.W_f_0(input_tensor)\n",
    "        h_c = self.W_c_0(input_tensor[:, :, :, 0])\n",
    "        \n",
    "        print('h_t: {}\\nh_f: {}\\nh_c: {}'.format(h_t.shape, h_f.shape, h_c.shape))\n",
    "        \n",
    "        for layer in self.module_list:\n",
    "            h_t, h_f, h_c = layer(h_t, h_f, h_c)\n",
    "            \n",
    "        theta_hat = self.W_theta(h_f)\n",
    "        \n",
    "        mu = theta_hat[:, :, :, :K]\n",
    "        std = torch.exp(theta_hat[:, :, :, K:2*K])\n",
    "        pi = self.pi_softmax(theta_hat[:, :, :, 2*K:])\n",
    "        \n",
    "#         loss = torch.tensor([0])\n",
    "        \n",
    "#         for batch in range(mu.shape[0]):\n",
    "#             for i in range(mu.shape[1]):\n",
    "#                 for j in range(mu.shape[2]):\n",
    "#                     prob = 0\n",
    "#                     for k in range(self.K):\n",
    "#                         prob += pi[batch, i, j, k] * torch.exp(torch.distributions.normal.Normal(mu[batch, i, j, k], std[batch, i, j, k]).log_prob(input_tensor[batch, i, j, 0]))\n",
    "\n",
    "#                     loss = torch.add(loss, -torch.log(prob))\n",
    "        \n",
    "        return h_t, h_f, h_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MelNet(512, 3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import scipy\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsc = 6 * 256\n",
    "hop = 256\n",
    "nov = nsc - hop\n",
    "n_mels = 256\n",
    "fs = 44100/2\n",
    "num_hidden = 512\n",
    "K = 10\n",
    "\n",
    "mel_filters = librosa.filters.mel(sr=fs, n_fft=nsc, n_mels=n_mels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data'\n",
    "file_list = glob.glob(data_dir + '/*')\n",
    "y, fs = librosa.core.load(file_list[0])\n",
    "\n",
    "f, t, Sxx = scipy.signal.stft(y, fs=fs, window='hann', nperseg=nsc, noverlap=nov)\n",
    "# Sxx = Sxx[1:, :]\n",
    "Zxx = np.abs(Sxx)\n",
    "log_spectrogram = 20 * np.log10(np.maximum(Zxx, 1e-8))\n",
    "log_spectrogram_norm = (log_spectrogram + 160) / 160\n",
    "\n",
    "mel_spectrogram = np.matmul(mel_filters, Zxx)\n",
    "log_mel_spectrogram = 20 * np.log10(np.maximum(mel_spectrogram, 1e-8))\n",
    "mel_input = (log_mel_spectrogram + 160) / 160\n",
    "\n",
    "Tier6 = mel_input[::2, :]\n",
    "Tier6_not = mel_input[1::2, :]\n",
    "\n",
    "Tier5 = Tier6_not[:, ::2]\n",
    "Tier5_not = Tier6_not[:, 1::2]\n",
    "\n",
    "Tier4 = Tier5_not[::2, :]\n",
    "Tier4_not = Tier5_not[1::2, :]\n",
    "\n",
    "Tier3 = Tier4_not[:, ::2]\n",
    "Tier3_not = Tier4_not[:, 1::2]\n",
    "\n",
    "Tier2 = Tier3_not[::2, :]\n",
    "Tier1 = Tier3_not[1::2, :]\n",
    "\n",
    "Tiers = [Tier1, Tier2, Tier3, Tier4, Tier5, Tier6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.tensor(Tier1.T)\n",
    "input_tensor = tensor.view([1, tensor.shape[0], tensor.shape[1], 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_h_t, _h_f, _h_c = net(input_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
